\documentclass[a4paper]{article}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}   % pour les images
\usepackage{hyperref}   % pour les références
\usepackage{amssymb}    % pour les symboles de maths comme \mathbb{R}
\usepackage{mathtools}  % pour rajouter \text dans un environment math
\usepackage{subcaption} % pour les subfigures
\usepackage{float}      % pour les figures

\usepackage[backend=biber]{biblatex}
\addbibresource{ref.bib}
% \usepackage[style=ieee]{biblatex}
% \addbibresource{ref.bib}
% \bibliography{ref.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,   % Couleur des liens internes (table des matières, références)
    citecolor=green,  % Couleur des liens vers les références bibliographiques
    filecolor=magenta,% Couleur des liens vers les fichiers
    urlcolor=blue     % Couleur des liens vers les URL
}

\title{Rapport SAM \\ Modèle multi-modaux}
\author{Cléa Han, Yanis Labeyrie et Adrien Zabban}
\date{janvier 2024}

\begin{document}

\maketitle

\section{Introduction}

Notre projet traite des approches multimodales pour la prédiction de changement de prise de parole dans des conversations naturelles. 
Nous nous appuyons sur le corpus de données multimodales appelé Paco-Cheese.
Les objectifs de ce projet nous permettent d'introduire différentes notions de modalité textuelle, visuelle et auditf, ainsi que de
comparer et d'explorer différents modèles de traitement multimodaux, et leur fusion entre eux.

\section{Données}

\input{sections/data.tex}

\section{Traitement unimodale}

\input{sections/model_unimodal.tex}

\section{Traitement multimodal}

\input{sections/model_multimodal.tex}

\section{Resultats}

\subsection{Métriques}

Pour comparer les résultats obtenu, nous avons utilisé la loss (la crossentropy), mais aussi d'autres métriques,
comme l'accuracy, la précision, le rappel et le $f_1$-score.

\subsection{Uni-modales}

% Nous avons entrainé les modèles \textit{TEXT}, \textit{AUDIO} et \textit{VIDEO}. Les deux premier modèles ont 
% été entrainé sur 5 epochs. La Figure~\ref{fig: train unimodale} montre la coubres d'accuracy d'entraînement 
% et de validation lors de l'entraînement de ces modèles.
Cependant, l'entraînement du modèle \textit{VIDEO}, du a la grandeur des données et 
leurs temps de chargement dans la RAM, nous avons décidé de faire qu'une seule epoch
\footnote{Une epoch durait plus de 2h30 sur google colab avec un GPU Nvidia T4.}.

La Table~\ref{tab: test unimodal} présente les résultats des modèles sur le corpus de test.

\input{sections/test_unimodal.tex}

\subsection{Multimodal}

Voici le résultat obtenu pour le modèle multimodal :

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}{0.45\textwidth}
%         \includegraphics[width=\textwidth]{../logs/multi_3/acc.png}
%         \caption{Accuracy of the multimodal model (text+audio) experiment.}
%         \label{fig:multi_1_acc}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.45\textwidth}
%         \includegraphics[width=\textwidth]{../logs/multi_3/crossentropy.png}
%         \caption{Cross-entropy loss of the multimodal model (text+audio) experiment.}
%         \label{fig:multi_1_loss}
%     \end{subfigure}
%     \caption{Results of the multimodal model (text+audio) experiment.}
%     \label{fig:multi_1_results}
% \end{figure}

% Les graphiques de la Figure \ref{fig:multi_1_results} présentent les résultats de l'expérience multimodale. L'axe des abscisses
% représente le nombre d'époques, et l'axe des ordonnées représente l'exactitude du modèle pour la Figure \ref{fig:multi_1_acc} et la
% perte en entropie croisée pour la Figure \ref{fig:multi_1_loss}.
% D'après ces graphiques, on peut déduire que le modèle est parvenu à apprendre à prédire les changements de tour de parole en tirant
% parti à la fois des données textuelles et audio.

\section{Conclusion}

Notre projet a exploré l'utilisation de modèles unimodaux et du multimodaux pour l'analyse dynamique de la conversation en français.
Nos modèles unimodaux ont été entraînés sur des données textuelles, audio et vidéo séparemment. Leur performance variaient selon le type de données, avec des résultats prometteurs pour les données textuelles et audio, mais des résultats moins satisfaisants pour les données vidéo.

Nos modèles multimodaux ont été entraînés en combinant les données textuelles et audio. 
% Les résultats de ces modèles étaient comparables à ceux des modèles unimodaux, ce qui indique que les données textuelles et audio contiennent des informations complémentaires.

% Ces résultats suggèrent que les modèles multimodaux, en particulier ceux qui combinent les données textuelles et audio, peuvent être des outils efficaces pour analyser la dynamique conversationnelle. 
% Cependant, la performance inférieure du modèle vidéo indique que des recherches supplémentaires sont nécessaires pour mieux comprendre comment intégrer et exploiter les données vidéo dans ces analyses.

Comme perspective, on pourrait explorer différentes manières d'intégrer les trois modalités de données (textes, audios et vidéos) dans un modèle multimodal, 
ainsi que l'utilisation d'autres types de données ou de modèles d'apprentissage automatique. 
De plus, des recherches supplémentaires pourraient également se concentrer sur l'amélioration des performances du modèle vidéo, 
peut-être en explorant différentes méthodes d'extraction de caractéristiques ou en utilisant des ensembles de données vidéo plus grands ou plus diversifiés.

\bigskip
\printbibliography

\end{document}
